\documentclass[11pt]{article}

\newcommand{\define}[2] {
  \textbf{Definition: #1}
  \begin{center} #2
\end{center}
}

\newcommand{\question}[2] {
  \textbf{Question: #1}
  \begin{center} #2
\end{center}
}

\begin{document}

\title{EXC Notes \\ S0936300}
\maketitle

\section{MapReduce}

Map reduce is a programming model for processing large data sets with a parallel, distributed algorithm on a cluster. It is not simply Map and Reduce functions(like in functional languages), but the scalability and fault-tolerance achieved for a variety of applications by optimizing the execution engine once \cite{wiki}.\\

\textbf{Properties:}
\begin{itemize}
\item Assumes large numbers of cheap, commodity machines
\item Failure is part of life
\item Tailored for dealing with Big Data
\item Simple
\item Scales well
\end{itemize}

\textbf{Who uses it?}

\begin{enumerate}
\item Google, Facebook, Twitter
\item IBM
\item Amazong Web Services
\item Edinburgh
\item Many small start-ups
\end{enumerate}

Map reduce is composed of the Map() function which performs filtering and sorting as well as the Reduce() function which performs a summary operation(such as counting the number of students in each queue).

\begin{enumerate}
\item Map(): Master node takes input, divides into smaller sub-problems and distributes these to the workers. Workers can sub-divide again. Once a worker has it's answer, it passes this to the master node.
\item Reduce(): Master node then collects answers to all the sub-problems and combines to form the output.
\end{enumerate}

\subsection{Critique}
Considerations as to whether MR can replace parallel databases. Parallel databases have been in development for over 20 years, they are robust, fast, scalable and based upon declarative data models.\\

MR is not really suited for low-latency problems as it's batch nature and lack of real-time guarentees means you shouldn't use it for front-end tasks. 

MR is not a good fit for problems which need global state information. Many Machine Learning algorithms require maintenance of centralised information and this implies a single task.

\textbf{Which application classes might MR be a better choice than a P-DB?}
\begin{itemize}
\item Extract-transform-load problems
\item Complex analytics
\item Semi-structured data(no single scheme for the data, I.e logs from multiple sources)
\item Quick and dirty analyses\\
\end{itemize}

\textbf{Results indicated}
\begin{itemize}
\item For a range of core tasks, P-DB was faster than Hadoop. P-DBS are flexible enough to deal with semi-structured data (unclear whether this is implementation specific)
\item Hadoop was criticised as being too low-level
\item Hadoop was easier for quick-and-dirty tasks. 
\begin{itemize}
\item Writing MR jobs can be easier than complex SQL queries. 
\item Non-specialists can quickly write MR jobs
\end{itemize}
\item Hadoop is a lot cheaper
\end{itemize}

\section{MapReduce2}

\subsection{Programming model}
MR offers one restricted version of parallel programming.

\begin{itemize}
\item Coarse-grained
\item No inter-process communication
\item Communication is (generally) through files
\end{itemize}

Input data is divided into \textbf{shards}. The map operation then works over the shards and emits key-value pairs(can be anything which can be represented as a string). 

Key-value pairs are then hashed. All those with the same hash-value are sent to the reducers. The input into the reducer is then sorted on it's key so that key-value pairs are locally grouped together.

Each mapper and reducer runs in parallel\cite{ques}. There is no state sharing between tasks and task communication is achieved using either external resources or at start-time. There need not be the same number of mappers as reducers. It is possible to have no reducers.

Tasks read their input sequentially as sequential disk reading is far more efficient than random access.

Reducing starts once mapping ends. Sorting and merging can be interleaved.

\begin{center}
\textbf{Example} :Count the number of words in a collection of documents.

Mapper: for each sentence, it emits the word and the value '1' 

Reducer: takes input and gives a partial count for it's input (see slide 15)

\end{center}

\subsection{Map Reduce Efficiency}

MR algorithms involve a lot of disk and network traffic:

\begin{itemize}
\item Typically start with big data
\item Mappers can produce intermediate results that are bigger than the input data
\item Input may not be on the same machine as that task which implies network traffic
\item Per-reducer input needs to be sorted.
\end{itemize}

Sharding might not produce a balanced set of inputs for each reducer, they are often skewed. Having an imbalanced set of inputs turns a parallel algorithm into a sequential one(why?)\cite{ques}.

Selecting the right number of mappers and reducers can greatly improve speed. More tasks mean that each task might fit in memory/require less network access and that failures are quicker to recover from. Fewer tasks mean having less of an over-head. But this is all a matter of guess-work.

Algorithmically, we can:
\begin{itemize}
\item Emit fewer key-value pairs: tasks can locally aggregate results and periodically emit them(combining)
\item Change the key: Key selection implies we partition the output. Some other selection might partition it more evenly.
\end{itemize}

\section{Hadoop}

Apache hadoop framework is composed of the following:
\begin{enumerate}
\item Hadoop Common: Contains libraries and utilities needed by other hadoop modules
\item HDFS: A distributed file-system that stores data on the commodity machines, providing very high aggregate bandwidth across the cluster. 
\item Hadoop YARN: A resource-management platform responsible for managing compute resources in clusters and using them for scheduling of user's applications.
\item MapReduce: a programming model for large scale data processing.
\end{enumerate}

All modules in hadoop are designed with the fundamental assumption that hardware failures are common and thus should be automatically handled in the software by the framework. Hadoops HDFS and Mapreduce were originally derived from google's HDFS and map-reduce. 

\subsection{HDFS}
Files are stored in HDFS (Hadoop File Distributed System). Files are stored as chunks or blocks which are by default 64mb. A file that is 150mb for example will be split up into blocks of 64:64:22.
Each block is given a unique name and number and stored on a different $data node$ in the cluster. The $namenode$ contains meta data as to where each block is stored and which blocks make up a file.\\
\textbf{Is there a problem?}
\begin{itemize}
\item Network failure
\item Disk failure on DN
\item Disk failure on NN
\item It is not a problem that block sizes may differ (i.e file split into 64, 64 where 22 differs)
\item It is not a problem if not all DN(datanodes) are used.
\end{itemize}

In order to deal with failure on DN, Hadoop replicates each block 3 times when stored in HDFS. When a datanode fails it isn't a problem as there are two other copies. Hadoop realises this is under-replicated and replicates the file again to ensure there are always 3 copies of the file available.

\textbf{What if there are problems on the NameNode?}
\begin{itemize}
\item If there is a network failure, data inaccessible
\item If there is a problem on the disc drive, the data is lost forever(how do we know where blocks are stored?)
\end{itemize}

One method was to store data on the NN and also NFS. Now there are commonly two name nodes an 'Active' name node and a 'Standby' name node which is used if the active NN fails.

\subsection{MapReduce}
Processing data in parallel can take a long time. MapReduce is designed to run in parallel. 

When you run a MR job, you submit the job to a Job Tracker. This splits the work into mappers and reducers which will run on the other cluster/datanodes. The actual MapReduce is handled by a daemon called the 'Task Tracker'. As a mapper is on the node, it will be able to work directly on the data that is stored at the machine which saves a lot of network traffic. If this isn't possible because the task tracker is already busy, it can send the data to another machine for processing although this happens rarely. The task tracker then sends the work to the reducers(shuffle and sort) which then process the data and store it back in HDFS.  

\subsubsection{Example}
Imagine you have stores all over the country and you're asked to calculate total sales per store.

Data:
2012-01-01 London Clothes 25.99
2012-01-01 Miami Music 12.15
2012-01-02 NYC Toys 3.10
2012-01-02 Miami Clothes 50.00

If run in parallel, we would calculate per line, get to the second 'Miami' and update our counter for Miami before continuing. Historically we could probably use a HashTable, with the $key$ as the store name and the value as the total sales but the problems with using this on big data is that we can run out of memory and it can take a long time having to read the entire file from disk and then process it line by line.

Using map reduce:
Take the file and break it into chunks which go to the mapper(this is a block/chunk of 64mb that was broken down onto HDFS earlier).
The mapper will then take each line in their chunk and categorise them by store. This is known as the intermediate records or data and will be in key-value form. In this example the key will be the store name and the value is the sales total for each individual input.
Shuffle and sort then takes place which is the movement of intermediate records from the mappers to the reducers.
The reducer is assigned a key(store name) and gets a list of values from all the mappers. It can then calculate the information per store.

How could we get the final result in sorted order? We require just one extra step(which merges the results) or one reducer(which doesn't scale very well).


\subsection{Lab 1}

\subsubsection*{Computing pi}
\begin{tabular}{l | l | l | l}
Number of maps & Number of samples & Time & Result \\
2 & 10 & 32.49 & 3.8 \\
5 & 10 & 32.29 & 3.28 \\
10 & 10 & 35.2 & 3.20 \\
2 & 100 & 32.367 & 3.12 \\
10 & 100 & 35.307 & 3.148
\end{tabular}

As the number of samples increases so does the accuracy. As the number of mappers increase, both the accuracy and time taken increases.

\subsubsection*{Word counting}


\section{Other concepts}

\subsection{BigTable}
BigTable is a form of Database.

\section{Definitions}

\define{Granularity}{Granularity is the extent in which a system is broken down into small parts. It is the extent to which a larger entity is sub-divided. For example a yard broken down into inches has finer granularity than a yard broken into feet}
\define{Coarse-grained}{Consist of few, larger components}
\define{Fine-grained}{Smaller components of which larger ones are composed}
\define{Cluster}{Large number of nodes(computers) that are on the same local network and use the same hardware}
\define{Moore's law}{Computing power doubles every 18 months.}
\define{Kryder's law}{Storage is growing even faster than Moore's law}
\define{Cell}{A grouping of servers, admins, users and clients}
\define{HDFS: Namenode}{The namenode contains information as to which chunks/blocks of files are stored where}
\define{HDFS: Datanode}{The datanode is where files/blocks in HDFS are stored}
\define{NFS}{Network File System. A method of mounting a remote disk.}
\define{Daemon}{A piece of code running all of the time on an individual node in a cluster}

\section{Questions}

\question{Do we use Hadoop with MapReduce?}{MapReduce libraries have been written in many programming languages with different levels of optimization. A popular open-source implementation is Apache Hadoop \cite{wiki}}

\question{Maps and Reducers run in parallel? Does it mean that they run together constantly or run one after the other? But this could still happen in parallel as long as each batch is waited on}{none}

\begin{thebibliography}{9}

\bibitem{wiki}
Wikipedia

\bibitem{udacity}
Udacity

\bibitem{Coursera}
Coursera

\bibitem{condense}
Re-write and remove detailed garble when know more on the topic.

\bibitem{ques}
There's a question about this

\end{thebibliography}

\end{document}
